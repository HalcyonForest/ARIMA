{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HFT_05_03.ipynb",
      "provenance": [],
      "mount_file_id": "1CtiI6nJK_aoD8K8EcN_tjOS4nHNi-cRS",
      "authorship_tag": "ABX9TyO7qLwHReyefHJg5DN94Afc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HalcyonForest/Time-Series-Forecasting/blob/main/HFT_05_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2w1z7-d0WhsC"
      },
      "source": [
        "import tensorflow\r\n",
        "import xgboost as xgb\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator # Generates batches for sequence data\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Dense,SimpleRNN,LSTM, Dropout, BatchNormalization\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping\r\n",
        "from keras.layers import RepeatVector\r\n",
        "from keras import optimizers\r\n",
        "from keras.layers import Bidirectional\r\n",
        "from tensorflow.keras import regularizers\r\n",
        "from keras.regularizers import l2\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import TimeDistributed\r\n",
        "from keras.layers.convolutional import Conv1D\r\n",
        "from keras.layers.convolutional import MaxPooling1D\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.metrics import mean_absolute_error\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import mean_squared_error as mse"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC9W2XIFW8AM"
      },
      "source": [
        "def split_sequences(sequences, n_steps_in, n_steps_out):\r\n",
        "  X, y = list(), list()\r\n",
        "  for i in range(len(sequences)):\r\n",
        "  # find the end of this pattern\r\n",
        "    end_ix = i + n_steps_in\r\n",
        "    out_end_ix = end_ix + n_steps_out-1\r\n",
        "  # check if we are beyond the dataset\r\n",
        "    if out_end_ix > len(sequences):\r\n",
        "      break\r\n",
        "    # gather input and output parts of the pattern\r\n",
        "    seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1:out_end_ix, -1]\r\n",
        "    X.append(seq_x)\r\n",
        "    y.append(seq_y)\r\n",
        "  return np.array(X), np.array(y)\r\n",
        "\r\n",
        "def tickSlicing(data, tickRate, windowSize, predictSize):\r\n",
        "  window_predict_in_seconds(tickRate, windowSize, predictSize)\r\n",
        "  scaler = MinMaxScaler()\r\n",
        "  data_scaled = df\r\n",
        "  data_scaled[['CLOSE', 'VOL']] = scaler.fit_transform(data_scaled[['CLOSE', 'VOL']])\r\n",
        "  data_scaled = data_scaled[::tickRate]\r\n",
        "  data = data[::tickRate]\r\n",
        "  #data_scaled = data_scaled[::tickRate]\r\n",
        "  #print(data_scaled.head())\r\n",
        "\r\n",
        "  X_close = np.reshape(np.array(data_scaled['CLOSE']), (len(data['CLOSE']),-1))\r\n",
        "  X_vol = np.reshape(np.array(data_scaled['VOL']), (len(data['VOL']),-1))\r\n",
        "  Y_close = np.reshape(np.array(data['CLOSE']), (len(data['CLOSE']), -1))\r\n",
        "  data = []\r\n",
        "  data = np.hstack((X_close, X_vol, Y_close))\r\n",
        "  # Переделать под слайсинг двух параметров - Close + vol upd:DONE\r\n",
        "  X, y = split_sequences(data, windowSize, predictSize)\r\n",
        "  return X, y, scaler\r\n",
        "\r\n",
        "def y_to_plot(y, y_step):\r\n",
        "  y = np.reshape(y[::y_step], (-1))\r\n",
        "  return y\r\n",
        "\r\n",
        "def window_predict_in_seconds(tickrate, window, horizon):\r\n",
        "  print(\"Смотрим каждый \", tickrate, \"-ый тик, за последние \",tickrate * window / 12 ,\"секунд\")\r\n",
        "  print(\"предсказываем на \",tickrate * horizon / 12 ,\" секунды вперед\")\r\n",
        "\r\n",
        "def error(X_test, y_test, model):\r\n",
        "  y_pred = model.predict(X_test)\r\n",
        "  y_pred = np.reshape(y_pred, (y_pred.shape[0], y_pred.shape[1]))\r\n",
        "  error = mean_squared_error(y_pred, y_test)\r\n",
        "  print(error)\r\n",
        "  return error\r\n",
        "\r\n",
        "def error_mae(X_test, y_test, model):\r\n",
        "  y_pred = model.predict(X_test)\r\n",
        "  y_pred = np.reshape(y_pred, (y_pred.shape[0], y_pred.shape[1]))\r\n",
        "  error = mean_absolute_error(y_pred, y_test)\r\n",
        "  print(error)\r\n",
        "  return error\r\n",
        "\r\n",
        "def plot_history(history_arr):\r\n",
        "  for history in history_arr:\r\n",
        "    plt.figure(figsize=(6,3))\r\n",
        "    plt.plot(history.history['loss'])\r\n",
        "    plt.plot(history.history['val_loss'])\r\n",
        "    plt.legend(['train', 'validation'])\r\n",
        "    plt.title('Loss')\r\n",
        "    plt.show()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW2twN-mWw_a"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/SBER_HFT_10D.csv')\r\n",
        "df['DATE'] = df['<DATE>']\r\n",
        "df['TIME'] = df['<TIME>']\r\n",
        "df['CLOSE'] = df['<LAST>']\r\n",
        "df['VOL'] = df['<VOL>']\r\n",
        "df = df.drop(columns=['<DATE>', '<TIME>','<LAST>', '<VOL>'])\r\n",
        "df['DATETIME'] = df['DATE'].astype(str) + df['TIME'].astype(str)\r\n",
        "df['DATETIME'].head()\r\n",
        "time = pd.to_datetime(df['DATETIME'], format='%Y%m%d%H%M%S')\r\n",
        "df.index = time\r\n",
        "df = df.drop(columns=['DATE', 'TIME', 'DATETIME'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIiFW5mAXAJi",
        "outputId": "57ec6a12-95be-4d8b-a126-726e9de79939"
      },
      "source": [
        "n_steps_in = 150 \r\n",
        "n_steps_out = 14\r\n",
        "tickrate = 2\r\n",
        "df = df.iloc[:df.shape[0] // 2]\r\n",
        "Xe, ye, scaler = tickSlicing(df, tickrate, n_steps_in, n_steps_out)\r\n",
        "n_input = Xe.shape[1] * Xe.shape[2]\r\n",
        "Xem = np.reshape(Xe, (Xe.shape[0], n_input))\r\n",
        "Xm_train,Xm_test,ym_train, ym_test = train_test_split(Xe, ye, test_size=0.4, shuffle=False) \r\n",
        "Xe.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Смотрим каждый  2 -ый тик, за последние  25.0 секунд\n",
            "предсказываем на  2.3333333333333335  секунды вперед\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(211110, 150, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNdAiYRuXA0E"
      },
      "source": [
        "def GetModel(nodes, layers, epochs, dropout, X_train, y_train, X_test, y_test):\r\n",
        "  model_lstm = Sequential()\r\n",
        "  model_lstm.add(LSTM(nodes, activation='relu', input_shape=(n_steps_in, Xe.shape[2]), return_sequences=(layers>1),\r\n",
        "                      kernel_regularizer=l2(0.00001), activity_regularizer=l2(0.00001)))\r\n",
        "  for i in range(1,layers):\r\n",
        "    model_lstm.add(LSTM(nodes // 2, activation='relu', return_sequences=(i < layers-1), recurrent_dropout=0.2, kernel_regularizer=l2(0.00001), \r\n",
        "               activity_regularizer=l2(0.00001) ))\r\n",
        "    nodes = nodes // 2\r\n",
        "  model_lstm.add(Dense((nodes**(2*layers))*10, activation='relu', kernel_regularizer=l2(0.000001)))\r\n",
        "  model_lstm.add(Dropout(dropout))\r\n",
        "  model_lstm.add(Dense(n_steps_out, activation='relu', kernel_regularizer=l2(0.000001)))\r\n",
        "  optAdam = optimizers.Adam(clipnorm=0.5, learning_rate = 0.0001, clipvalue=0.5)\r\n",
        "  # optSGD = optimizers.SGD(lr=0.001, momentum=0.9, clipnorm=0.5, clipvalue=1.0)\r\n",
        "  model_lstm.compile(optimizer=optAdam, loss='mae',\r\n",
        "    #metrics=[tf.keras.metrics.MeanAbsolutePercentageError]\r\n",
        "    )\r\n",
        "  history = model_lstm.fit(X_train, y_train, epochs = epochs, verbose=1, batch_size = 1500, validation_data=(X_test, y_test), callbacks=tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5))\r\n",
        "  err_mlp = error_mae(X_test, y_test, model_lstm)\r\n",
        "  return history, err_mlp\r\n",
        "\r\n",
        "def NodeSearch(params, X_train, y_train, X_test, y_test):\r\n",
        "  hys = []\r\n",
        "  errors = []\r\n",
        "  for param in params:\r\n",
        "    hystory, error = GetModel(param, layers=1, epochs=100, dropout=0.35, X_train=X_train, y_train=y_train , X_test=X_test, y_test=y_test)\r\n",
        "    hys.append(hystory)\r\n",
        "    errors.append(error)\r\n",
        "    print(\"Nodes: \", param, \"MAE error: \", error)\r\n",
        "  return hys, errors\r\n",
        "\r\n",
        "def LayersSearch(params, X_train, y_train, X_test, y_test):\r\n",
        "  hys = []\r\n",
        "  errors = []\r\n",
        "  for param in params:\r\n",
        "    hystory, error = GetModel(nodes=30, layers=param, epochs=60, dropout=0.35, X_train=X_train, y_train=y_train , X_test=X_test, y_test=y_test)\r\n",
        "    hys.append(hystory)\r\n",
        "    errors.append(errors)\r\n",
        "    print(\"Layers: \", param, \"MAE error: \", error)\r\n",
        "  return hys, errors"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HA7YaLCZY77"
      },
      "source": [
        "nodes = [10, 20, 30, 40, 50]\r\n",
        "layers = [2,3,4]\r\n",
        "epochs = [50,100,150]\r\n",
        "dropout = [0.1,0.25,0.4]\r\n",
        "batch_size = [10,20,50,100]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krEH_TgcZGHI"
      },
      "source": [
        "%%time\r\n",
        "nodes_hys, nodes_errors = NodeSearch(nodes, Xm_train, ym_train, Xm_test, ym_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspPD8kzZIDD",
        "outputId": "1f5d0568-85c5-494b-addb-3352fb0e0c99"
      },
      "source": [
        "%%time\r\n",
        "layerss_hys, layers_errors = LayersSearch(layers, Xm_train, ym_train, Xm_test, ym_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlKV3b8kZOWv"
      },
      "source": [
        "print(nodes_errors)\r\n",
        "#plot_history(nodes_hys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WRzY2B4ZLEU"
      },
      "source": [
        "print(layers_errors)\r\n",
        "#plot_history(layerss_hys)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}